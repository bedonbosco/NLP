[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services)

## Table of Contents

* [**Spelling Correction**](#spelling-correction)
* [**Word Segmentation**](#word-segmentation)
* [**Part-of-Speech Tagging**](#part-of-speech-tagging)
* [**Chunking**](#chunking)
* [**Parsing**](#parsing)
* [**Representation**](#representation)
* [**Text Classification**](#text-classification)
* [**Automatic Summarization**](#automatic-summarization)
* [**Sentiment Analysis**](#sentiment-analysis)
* [**Named Entity Recognition**](#named-entity-recognition)
* [**Coreference Resolution**](#coreference-resolution)
* [**Relationship Extraction**](#relationship-extraction)
* [**Semantic Role Labeling**](#semantic-role-labeling)
* [**Machine Translation**](#machine-translation)
* [**Question Answering**](#question-answering)
* [**Language Generation**](#language-generation)

# Part-of-Speech tagging

[`⟰ Publication`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#part-of-speech-tagging) | **`☶ SOTA`**

**[Wall Street Journal Section of Penn Treebank](https://github.com/magizbox/underthesea/wiki/POS-Tagging-Task:-WSJ-PTB)** <sub><sup>8 results collected</sup></sub>

The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC1999T42) and Treebank-3 (LDC1999T42) releases of PTB. Treebank-2 includes the raw text for each story. Three "map" files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER.

# Sentiment Analysis

[`⟰ Publication`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#sentiment-analysis) | **`☶ SOTA`**

**[SemEval-2017 Task 5](https://github.com/magizbox/underthesea/wiki/SemEval-2017-Task-5)** <sub><sup>31 results collected</sup></sub>

The proposed task aims at catalysing discussions around approaches of semantic interpretation of financial texts by targeting a concrete sentiment analysis task, which identifies bullish (optimistic; believing that the stock price will increase) and bearish (pessimistic; believing that the stock price will decline) sentiment associated with companies and stocks.

**[SemEval-2016 Task 5](https://github.com/magizbox/underthesea/wiki/SemEval-2016-Task-5)** <sub><sup>245 results collected</sup></sub>

The SemEval ABSA task for 2016 (SE-ABSA16) gives the opportunity to participants to further experiment with English data (reviews) from the domains of SE-ABSA15 (laptops, restaurants, hotels) by providing new test datasets. In addition, SE-ABSA16 will also provide datasets in other than English languages. For each domain (e.g. restaurants) a common set of annotation guidelines will be used across all languages. Furthermore, SE-ABSA16 includes text-level annotations along with a suitable evaluation methodology.

**[SemEval-2015 Task 12](https://github.com/magizbox/underthesea/wiki/SemEval-2015-Task-12)** <sub><sup>32 results collected</sup></sub>

SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews
in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.

**[SemEval-2014 Task 4](https://github.com/magizbox/underthesea/wiki/SemEval-2014-Task-4)** <sub><sup>16 results collected</sup></sub>

Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.