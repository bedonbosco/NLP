[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services)

## Table of Contents

* [**Preprocess**](#preprocess)
* [**Sentence Boundary Disambiguation**](#sentence-boundary-disambiguation)
* [**Language Detection**](#language-detection)
* [**Spelling Correction**](#spelling-correction)
* [**Word Segmentation**](#word-segmentation)
* [**Part-of-Speech Tagging**](#part-of-speech-tagging)
* [**Chunking**](#chunking)
* [**Parsing**](#parsing)
* [**Representation**](#representation)
* [**Text Classification**](#text-classification)
* [**Sentiment Analysis**](#sentiment-analysis)
* [**Automatic Summarization**](#automatic-summarization)
* [**Named Entity Recognition**](#named-entity-recognition)
* [**Entity Linking**](#entity-linking)
* [**Semantics**](#semantics)
* [**Coreference Resolution**](#coreference-resolution)
* [**Relationship Extraction**](#relationship-extraction)
* [**Semantic Role Labeling**](#semantic-role-labeling)
* [**Machine Translation**](#machine-translation)
* [**Question Answering**](#question-answering)
* [**Language Generation**](#language-generation)
* [**Automatic Speech Recognition**](#automatic-speech-recognition)
* [**Text To Speech**](#text-to-speech)
* [**Resources**](#resources)

## Part-of-Speech Tagging

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#part-of-speech-tagging) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#part-of-speech-tagging) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#part-of-speech-tagging)

**[Wall Street Journal Section of Penn Treebank](https://github.com/magizbox/underthesea/wiki/POS-Tagging-Task:-WSJ-PTB)** <sub><sup>8 results collected</sup></sub>

The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC1999T42) and Treebank-3 (LDC1999T42) releases of PTB. Treebank-2 includes the raw text for each story. Three "map" files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER.

## Chunking

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#chunking) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#chunking) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#chunking)

**[Wall Street Journal Section of Penn Treebank](https://github.com/magizbox/underthesea/wiki/Chunking-Task:-WSJ-PTB)** <sub><sup>3 results collected</sup></sub>

The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. These 2,499 stories have been distributed in both Treebank-2 (LDC1999T42) and Treebank-3 (LDC1999T42) releases of PTB. Treebank-2 includes the raw text for each story. Three "map" files are available in a compressed file (pennTB_tipster_wsj_map.tar.gz) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER.

## Representation

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#representation) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#representation) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#representation)

**[Simverb-3500-Dataset](https://github.com/magizbox/underthesea/wiki/DATA-SIMVERB-3500)** <sub><sup>5 results collected</sup></sub>

SimVerb-3500 (Gerz ‎2016) is an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class.

**[SimLex-999 Dataset](https://github.com/magizbox/underthesea/wiki/DATA-SIMLEX-999)** <sub><sup>14 results collected</sup></sub>

SimLex-999 (Hill 2014) is a gold standard resource for the evaluation of models that learn the meaning of words and concepts. SimLex-999 provides a way of measuring how well models capture similarity, rather than relatedness or association.

**[Rare Word (RW) Dataset](https://github.com/magizbox/underthesea/wiki/DATA-RAREWORD)** <sub><sup>15 results collected</sup></sub>

Most existing word similarity datasets contain frequent words and few of them possesses enough rare or morphologically complex words that we could really attest the expressiveness of our morphoRNN models. Good embedding in general should be able to learn useful representations for not just frequent words but also rare ones. Rare Word (Luong 2013) is dataset focusing on rare words to complement existing ones.

**[WordSim353 Dataset](https://github.com/magizbox/underthesea/wiki/DATA-WORDSIM-353)** <sub><sup>22 results collected</sup></sub>

The WordSimilarity-353 Test Collection (Gabrilovich 2002) contains two sets of English word pairs along with human-assigned similarity judgements. The collection can be used to train and/or test computer algorithms implementing semantic similarity measures.

## Text Classification

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#text-classification) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#text-classification) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#text-classification)

**[AG’s news corpus](https://github.com/magizbox/underthesea/wiki/DATA-AGNEWS)** <sub><sup>22 results collected</sup></sub>

AG (GM Del Corso et al. 2005) is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. 

**[TREC question dataset](https://github.com/magizbox/underthesea/wiki/DATA-TREC)** <sub><sup>6 results collected</sup></sub>

Task involves classifying a question into 6 question types (whether the question is about person,
location, numeric information, etc.) (Li et al. 2002)

## Sentiment Analysis

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#sentiment-analysis) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#sentiment-analysis) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#sentiment-analysis)

**[SemEval-2016 Task 5](https://github.com/magizbox/underthesea/wiki/SemEval-2016-Task-5)** <sub><sup>245 results collected</sup></sub>

The SemEval ABSA task for 2016 (SE-ABSA16) gives the opportunity to participants to further experiment with English data (reviews) from the domains of SE-ABSA15 (laptops, restaurants, hotels) by providing new test datasets. In addition, SE-ABSA16 will also provide datasets in other than English languages. For each domain (e.g. restaurants) a common set of annotation guidelines will be used across all languages. Furthermore, SE-ABSA16 includes text-level annotations along with a suitable evaluation methodology.

**[SemEval-2015 Task 12](https://github.com/magizbox/underthesea/wiki/SemEval-2015-Task-12)** <sub><sup>32 results collected</sup></sub>

SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews
in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.

**[SemEval-2014 Task 4](https://github.com/magizbox/underthesea/wiki/SemEval-2014-Task-4)** <sub><sup>16 results collected</sup></sub>

Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.

**[Stanford Sentiment Treebank](https://github.com/magizbox/underthesea/wiki/DATA-SST)** <sub><sup>15 results collected</sup></sub>

Stanford Sentiment Treebank—an extension of MR but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by Socher et al. (2013).

**[MPQA Opinion Corpus](https://github.com/magizbox/underthesea/wiki/DATA-MPQA)** <sub><sup>9 results collected</sup></sub>

The MPQA Opinion Corpus (Wiebe et al. 2005) contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.)

## Named Entity Recognition

[`⚙ Tools`](https://github.com/magizbox/underthesea/wiki/English-NLP-Tools#named-entity-recognition) | [`⟰ Publications`](https://github.com/magizbox/underthesea/wiki/English-NLP-Publications#named-entity-recognition) | **`☶ SOTA`** | [`⚛ Services`](https://github.com/magizbox/underthesea/wiki/English-NLP-Services#named-entity-recognition)

**[CoNLL-2012 Shared Task: Language-Independent Named Entity Recognition](https://github.com/magizbox/underthesea/wiki/TASK-CONLL-2012)** <sub><sup>4 results collected</sup></sub>

The OntoNotes Release 5.0 shared task deals with language-independent named entity recognition as well (English, Chinese, and Arabi).

**[CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://github.com/magizbox/underthesea/wiki/TASK-CONLL-2003)** <sub><sup>3 results collected</sup></sub>

The CoNLL-2003 (Sang et al. 2003) shared task deals with language-independent named entity recognition as well (English and German).